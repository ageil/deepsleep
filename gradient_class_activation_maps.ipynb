{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and helper functions\n",
    "\n",
    "Based on https://github.com/jacobgil/keras-grad-cam\n",
    "\n",
    "**Note**: Requires Keras <= 2.0.8 (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T00:27:16.292153Z",
     "start_time": "2017-12-14T00:26:56.402112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "import cv2 \n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x, data_format='channels_last')\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name, num_classes, multi=False):\n",
    "    model = Sequential()\n",
    "    model.add(input_model)\n",
    "\n",
    "    nb_classes = num_classes\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "    if multi:\n",
    "        conv_output =  [l for l in model.layers[0].layers if l.name is layer_name][0].get_output_at(1)\n",
    "    else:\n",
    "        conv_output =  [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
    "    \n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on VGG model with ImageNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T00:27:28.733832Z",
     "start_time": "2017-12-14T00:27:16.296472Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg = VGG16(weights='imagenet')\n",
    "\n",
    "for img in [\"img_1\", \"img_336\"]:\n",
    "    preprocessed_input = load_image(img + \".png\")\n",
    "\n",
    "    predictions = vgg.predict(preprocessed_input)\n",
    "#     top_1 = decode_predictions(predictions)[0][0]\n",
    "#     print('Predicted class:')\n",
    "#     print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    cam, heatmap = grad_cam(vgg, preprocessed_input, predicted_class, \"block5_conv3\", 1000)\n",
    "    cv2.imwrite(\"gcam_\" + img + \"_VGG.jpg\", cam)\n",
    "\n",
    "    register_gradient()\n",
    "    guided_model = modify_backprop(vgg, 'GuidedBackProp')\n",
    "    saliency_fn = compile_saliency_function(guided_model)\n",
    "    saliency = saliency_fn([preprocessed_input, 0])\n",
    "    gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "    cv2.imwrite(\"guided_gcam_\" + img + \"_VGG.jpg\", deprocess_image(gradcam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on VGG model with our own weights \n",
    "\n",
    "Remember to disable dropoff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T00:27:32.137299Z",
     "start_time": "2017-12-14T00:27:28.736160Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.initializers as initializers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_dense = 4096\n",
    "num_classes = 5\n",
    "\n",
    "def build_model(init_seed=None):\n",
    "    # CONVOLUTIONAL LAYERS: use the pretrained weights from ImageNet\n",
    "    base_model = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
    "    \n",
    "    # TOP LAYERS: Trainable dense layers with random initialization\n",
    "    dense_model = Sequential()\n",
    "    # Input layer\n",
    "    dense_model.add(Flatten(input_shape=base_model.output_shape[1:], name=\"Flatten\"))\n",
    "    # FC6\n",
    "    dense_model.add(Dense(num_dense, activation='relu', kernel_initializer=initializers.glorot_normal(seed=init_seed), kernel_regularizer=regularizers.l2(0.01), name=\"Dense1\"))\n",
    "    # FC6 Dropout\n",
    "#     dense_model.add(Dropout(rate=0.5, name=\"Dropout1\"))\n",
    "    # FC7\n",
    "    dense_model.add(Dense(num_dense, activation='relu', kernel_initializer=initializers.glorot_normal(seed=init_seed), kernel_regularizer=regularizers.l2(0.01), name=\"Dense2\"))\n",
    "    # FC7 Dropout\n",
    "#     dense_model.add(Dropout(rate=0.5, name=\"Dropout2\"))\n",
    "    # Softmax\n",
    "    dense_model.add(Dense(num_classes, activation='softmax', kernel_initializer=initializers.glorot_normal(seed=init_seed), kernel_regularizer=regularizers.l2(0.01), name=\"Dense3\"))\n",
    "\n",
    "    # CREATE THE FULL MODEL (stack the dense layers on the convolutional layers)\n",
    "    model = Sequential()\n",
    "    # Add convolutional layers\n",
    "    for layer in base_model.layers:\n",
    "        model.add(layer)\n",
    "    # Fix weights in the convolutional layers\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # Add fully conncected layers\n",
    "    for layer in dense_model.layers:\n",
    "        model.add(layer)\n",
    "    \n",
    "    \n",
    "    # COMPILE THE MODEL\n",
    "    # Optimizer (can't figure out what learning rate Albert used)\n",
    "    adam = Adam(lr = 0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "cnn = build_model(3)\n",
    "cnn.load_weights('weights_fold0.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T00:27:42.331186Z",
     "start_time": "2017-12-14T00:27:32.139508Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img in [\"img_1\", \"img_336\"]:\n",
    "    preprocessed_input = load_image(img+\".png\")\n",
    "\n",
    "    predictions = cnn.predict(preprocessed_input)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "\n",
    "    cam, heatmap = grad_cam(cnn, preprocessed_input, predicted_class, \"block5_conv3\", 5, multi=True)\n",
    "    cv2.imwrite(\"gcam_\" + img + \".jpg\", cam)\n",
    "\n",
    "    register_gradient()\n",
    "    guided_model = modify_backprop(cnn, 'GuidedBackProp')\n",
    "    saliency_fn = compile_saliency_function(guided_model)\n",
    "    saliency = saliency_fn([preprocessed_input, 0])\n",
    "    gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "    cv2.imwrite(\"guided_gcam_\" + img + \".jpg\", deprocess_image(gradcam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T23:46:44.627081Z",
     "start_time": "2017-12-13T23:46:44.623675Z"
    }
   },
   "source": [
    "# TODO\n",
    "\n",
    "- find way to visualize LCRN model\n",
    "- loop over all images for one night with cnn.predict, get predicted_class, output to predctions list\n",
    "- plot time vs predictions list and compare with time vs true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
