{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo notebook for the CRNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Note, we are using Python 3.6.2 with TensorFlow 1.3.0 and Keras 2.0.8 on MacOS version 10.13.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:13:59.862456Z",
     "start_time": "2017-12-21T14:13:48.394068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.initializers as initializers\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import regularizers\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "from skimage import io\n",
    "import skimage.transform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set basic hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:13:59.878312Z",
     "start_time": "2017-12-21T14:13:59.864469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of output classes\n",
    "num_classes = 5\n",
    "# Number of subjects\n",
    "num_subjects = 20\n",
    "# Number of units in dense layers\n",
    "num_dense = 4096\n",
    "# Number of units in LSTM layers\n",
    "num_lstm = 128\n",
    "# Name of sensor used\n",
    "sensors = 'fpz'\n",
    "# Path to spectrograms\n",
    "impath = '../imdata/'\n",
    "# Model name (for output filenames)\n",
    "mdname = 'LCRN1'\n",
    "# Folds used for training\n",
    "fmin, fmax = 4, 4\n",
    "# Intiialization seed\n",
    "init_seed = 3\n",
    "# Numer of epochs to run\n",
    "n_epochs = 1\n",
    "# Size of batches\n",
    "batch_size = 16\n",
    "# Number of images per sample \n",
    "# (i.e. for tsteps = 5, 2 context images on either side of classified image)\n",
    "tsteps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:13:59.905196Z",
     "start_time": "2017-12-21T14:13:59.880667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class weight dictionary used to account for imbalanced data\n",
    "class_weights = {0: 1.69217121,\n",
    "                 1: 2.76249095,\n",
    "                 2: 0.43409,\n",
    "                 3: 1.36469326,\n",
    "                 4: 0.98949553}\n",
    "\n",
    "# Fold used for cross validation: test = idx 0:1; validation = idx 2:5; train = idx 6:19\n",
    "# (Corresponding to our 20 test subjects)\n",
    "folds = np.array([[10,14,1,13,15,19,12,17,5,9,2,4,16,3,11,18,8,20,7,6],\n",
    "                  [20,18,13,9,3,11,16,1,19,5,4,14,7,6,15,17,2,12,8,10],\n",
    "                  [16,13,2,9,15,8,18,14,7,17,19,6,11,20,3,1,4,5,10,12],\n",
    "                  [17,4,1,20,5,16,8,7,2,18,10,15,14,19,13,12,9,11,3,6],\n",
    "                  [2,11,6,15,8,12,5,4,14,13,7,19,3,10,9,18,16,17,1,20],\n",
    "                  [19,6,10,2,11,15,7,3,9,13,5,20,17,4,1,14,8,18,16,12],\n",
    "                  [15,9,5,6,13,19,2,17,18,3,11,12,4,20,1,10,14,16,7,8],\n",
    "                  [5,12,9,1,17,13,15,11,20,19,18,7,6,14,4,8,10,16,3,2],\n",
    "                  [8,3,10,16,12,1,6,15,20,13,19,18,4,14,9,11,7,2,17,5],\n",
    "                  [1,7,3,15,10,6,18,4,5,12,19,20,2,13,16,14,8,9,11,17]])\n",
    "folds = folds-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:14:00.205282Z",
     "start_time": "2017-12-21T14:13:59.907842Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_model(init_seed=None, cnn_softmax=False, timesteps=tsteps, droprate=0.5):\n",
    "    # load weights data\n",
    "    wpath = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    wpath_notop = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                        wpath,\n",
    "                                        cache_subdir='models',\n",
    "                                        file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "    weights_path_no_top = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                        wpath_notop,\n",
    "                                        cache_subdir='models',\n",
    "                                        file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "\n",
    "    # CNN (TimeDistributed VGG16)\n",
    "    cnn = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    cnn.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same', trainable=False), name='block1_tdist_conv1', input_shape=(timesteps, 224, 224, 3))) # timesteps x 224x224x64\n",
    "    cnn.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same', trainable=False), name='block1_tdist_conv2')) # timesteps x 224x224x64\n",
    "    cnn.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)), name='block1_tdist_pool')) # timesteps x 112x112x64\n",
    "\n",
    "    # Block 2\n",
    "    cnn.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same', trainable=False), name='block2_tdist_conv1')) # timesteps x 112x112x128\n",
    "    cnn.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same', trainable=False), name='block2_tdist_conv2')) # timesteps x 112x112x128\n",
    "    cnn.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)), name='block2_tdist_pool')) # timesteps x 56x56x128\n",
    "\n",
    "    # Block 3\n",
    "    cnn.add(TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', trainable=False), name='block3_tdist_conv1')) # timesteps x 56x56x256\n",
    "    cnn.add(TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', trainable=False), name='block3_tdist_conv2')) # timesteps x 56x56x256\n",
    "    cnn.add(TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', trainable=False), name='block3_tdist_conv3')) # timesteps x 56x56x256\n",
    "    cnn.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)), name='block3_tdist_pool')) # timesteps x 28x28x256\n",
    "\n",
    "    # Block 4\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block4_tdist_conv1')) # timesteps x 28x28x512\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block4_tdist_conv2')) # timesteps x 28x28x512\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block4_tdist_conv3')) # timesteps x 28x28x512\n",
    "    cnn.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)), name='block4_tdist_pool')) # timesteps x 14x14x512\n",
    "\n",
    "    # Block 5\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block5_tdist_conv1')) # timesteps x 14x14x512\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block5_tdist_conv2')) # timesteps x 14x14x512\n",
    "    cnn.add(TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', trainable=False), name='block5_tdist_conv3')) # timesteps x 14x14x512\n",
    "    cnn.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)), name='block5_tdist_pool')) # timesteps x 7x7x512\n",
    "\n",
    "    # RNN (bidirectional LSTM)\n",
    "    rnn = Sequential()\n",
    "    rnn.add(TimeDistributed(Flatten(), input_shape=cnn.output_shape[1:], name=\"block6_tdist_flatten\")) # timesteps x 25088\n",
    "    rnn.add(Bidirectional(LSTM(32, return_sequences=True), name=\"block6_bidir_lstm1\")) # timesteps x (2x32)\n",
    "    rnn.add(Dropout(rate=droprate, name=\"block6_dropout1\")) # timesteps x (2x32)\n",
    "    rnn.add(Bidirectional(LSTM(64, return_sequences=True), name=\"block6_bidir_lstm2\")) # timesteps x (2x64)\n",
    "    rnn.add(Dropout(rate=droprate, name=\"block6_dropout2\")) # timesteps x (2x64)\n",
    "    rnn.add(LSTM(num_classes, activation='softmax', return_sequences=False, name=\"block6_lstm3\")) # 1 x num_classes\n",
    "\n",
    "    # Combine to LCRN\n",
    "    lcrn = Sequential()\n",
    "    # add VGG 16 base layers\n",
    "    for layer in cnn.layers:\n",
    "        lcrn.add(layer)\n",
    "    # load VGG16 imagenet weights\n",
    "    lcrn.load_weights(weights_path_no_top) # only base layers\n",
    "    # add rnn layers\n",
    "    for layer in rnn.layers:\n",
    "        lcrn.add(layer)\n",
    "\n",
    "\n",
    "    # COMPILE THE MODEL\n",
    "    adam = Adam(lr = 0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    lcrn.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=adam,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return lcrn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions used during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a few training data samples in `./databatches/` and a test sample for the pretrained weights in `./testbatch/`. \n",
    "\n",
    "To see how these batches were generated, please refer to `../batch_data.py`. To see how these raw images were generated, please refer to Albert Vilamala's implementation in `../EEGtoRGB.m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:14:00.228864Z",
     "start_time": "2017-12-21T14:14:00.207191Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data generator used for training data\n",
    "def data_gen(paths):\n",
    "    while 1:\n",
    "        paths = random.sample(paths, len(paths)) # shuffle between epochs\n",
    "        for file in paths:\n",
    "            with open(file, 'rb') as f:\n",
    "                x, y = pickle.load(f)\n",
    "            y_cat = np.argmax(y, axis=1)\n",
    "            batch_weights = {key: class_weights[key] for key in np.unique(y_cat)}\n",
    "            sample_weights = class_weight.compute_sample_weight(batch_weights, y_cat, indices=None)\n",
    "            yield x, y, sample_weights\n",
    "\n",
    "# Data generator used for testing data\n",
    "def data_gen_test(paths):\n",
    "    while 1:\n",
    "        for file in paths:\n",
    "            with open(file, 'rb') as f:\n",
    "                x, y = pickle.load(f)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:14:00.267832Z",
     "start_time": "2017-12-21T14:14:00.231444Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that plots training and validation error/accuracy\n",
    "def plot_training_history(history, fold, plotpath, show=False):\n",
    "                \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    filename = plotpath+'/fold' + str(fold) + '_acc.png'\n",
    "    plt.savefig(filename, dpi=72)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    filename = plotpath+'/fold' + str(fold) + '_loss.png'\n",
    "    plt.savefig(filename, dpi=72)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:14:00.369152Z",
     "start_time": "2017-12-21T14:14:00.271411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom Keras Callback class to continuously calculate test error.\n",
    "# Error is only calculated if the epoch has lower validation error than all previous epochs. \n",
    "# Test error and accuracy can be accessed through TestOnBest.history       \n",
    "class TestOnBest(Callback):\n",
    "    # Initialize dict to store test loss and accuracy, and some tracker variables\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        self.best_loss = float('inf')\n",
    "        self.current_loss = float('inf')\n",
    "        self.best_weights = []\n",
    "        self.confusion_matrix = []\n",
    "        self.history = {'test_acc': []}\n",
    "\n",
    "    # At the end of epoch, check if validation loss is the best so far    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.current_loss = logs.get('val_loss')\n",
    "        \n",
    "        # If validation error is lowest, compute test error and store in dict\n",
    "        if (self.current_loss < self.best_loss):\n",
    "            \n",
    "            self.best_loss = self.current_loss\n",
    "            test_paths = self.test_data\n",
    "\n",
    "            y_pred = np.empty((1,num_classes))\n",
    "            y = np.empty((1,num_classes))\n",
    "            # Get predictions\n",
    "            for file in test_paths:\n",
    "                with open(file, 'rb') as f:\n",
    "                    x, y_batch = pickle.load(f)\n",
    "                ypred_batch = self.model.predict_on_batch(x)\n",
    "                y_pred = np.concatenate((y_pred, ypred_batch))\n",
    "                y = np.concatenate((y, y_batch))\n",
    "            y_pred = y_pred[1:,:]\n",
    "            y = y[1:,:]\n",
    "            \n",
    "            # probability to hard class assignment\n",
    "            y_pred = to_categorical(np.argmax(y_pred, axis=1), num_classes=num_classes)\n",
    "\n",
    "            # Calculate accuracy (checked: equals accuracy obtained from model.fit())\n",
    "            acc = y_pred + y\n",
    "            acc = np.sum(acc==2, dtype='float32')/np.shape(acc)[0]\n",
    "            print('Test acc: {}\\n'.format(acc))\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            # Convert from one-hot to integer labels\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            m = confusion_matrix(y, y_pred, labels=None, sample_weight=None)\n",
    "            self.confusion_matrix = m\n",
    "\n",
    "            # Get model weights and store in temporary variable\n",
    "            self.best_weights = model.get_weights()\n",
    "        else:\n",
    "            self.history['test_acc'].append(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training loop\n",
    "\n",
    "(This might take ~3 minutes to run on a laptop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:20:32.612333Z",
     "start_time": "2017-12-21T14:17:46.710180Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_tdist_conv1 (TimeDist (None, 5, 224, 224, 64)   1792      \n",
      "_________________________________________________________________\n",
      "block1_tdist_conv2 (TimeDist (None, 5, 224, 224, 64)   36928     \n",
      "_________________________________________________________________\n",
      "block1_tdist_pool (TimeDistr (None, 5, 112, 112, 64)   0         \n",
      "_________________________________________________________________\n",
      "block2_tdist_conv1 (TimeDist (None, 5, 112, 112, 128)  73856     \n",
      "_________________________________________________________________\n",
      "block2_tdist_conv2 (TimeDist (None, 5, 112, 112, 128)  147584    \n",
      "_________________________________________________________________\n",
      "block2_tdist_pool (TimeDistr (None, 5, 56, 56, 128)    0         \n",
      "_________________________________________________________________\n",
      "block3_tdist_conv1 (TimeDist (None, 5, 56, 56, 256)    295168    \n",
      "_________________________________________________________________\n",
      "block3_tdist_conv2 (TimeDist (None, 5, 56, 56, 256)    590080    \n",
      "_________________________________________________________________\n",
      "block3_tdist_conv3 (TimeDist (None, 5, 56, 56, 256)    590080    \n",
      "_________________________________________________________________\n",
      "block3_tdist_pool (TimeDistr (None, 5, 28, 28, 256)    0         \n",
      "_________________________________________________________________\n",
      "block4_tdist_conv1 (TimeDist (None, 5, 28, 28, 512)    1180160   \n",
      "_________________________________________________________________\n",
      "block4_tdist_conv2 (TimeDist (None, 5, 28, 28, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "block4_tdist_conv3 (TimeDist (None, 5, 28, 28, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "block4_tdist_pool (TimeDistr (None, 5, 14, 14, 512)    0         \n",
      "_________________________________________________________________\n",
      "block5_tdist_conv1 (TimeDist (None, 5, 14, 14, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "block5_tdist_conv2 (TimeDist (None, 5, 14, 14, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "block5_tdist_conv3 (TimeDist (None, 5, 14, 14, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "block5_tdist_pool (TimeDistr (None, 5, 7, 7, 512)      0         \n",
      "_________________________________________________________________\n",
      "block6_tdist_flatten (TimeDi (None, 5, 25088)          0         \n",
      "_________________________________________________________________\n",
      "block6_bidir_lstm1 (Bidirect (None, 5, 64)             6430976   \n",
      "_________________________________________________________________\n",
      "block6_dropout1 (Dropout)    (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "block6_bidir_lstm2 (Bidirect (None, 5, 128)            66048     \n",
      "_________________________________________________________________\n",
      "block6_dropout2 (Dropout)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "block6_lstm3 (LSTM)          (None, 5)                 2680      \n",
      "=================================================================\n",
      "Total params: 21,214,392\n",
      "Trainable params: 6,499,704\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "Test acc: 0.0625\n",
      "\n",
      "150s - loss: 2.9071 - acc: 0.0938 - val_loss: 2.6988 - val_acc: 0.1250\n"
     ]
    }
   ],
   "source": [
    "# Make sure output paths are in place\n",
    "plotpath = '../plots'\n",
    "if not os.path.exists(plotpath):\n",
    "        os.makedirs(plotpath)   \n",
    "outpath = '../outputs'\n",
    "if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "\n",
    "# Build model, save initial weights\n",
    "model = build_model(init_seed)\n",
    "# print summary\n",
    "model.summary()\n",
    "# save initial weights\n",
    "Winit = model.get_weights()\n",
    "\n",
    "for fold in range(fmin, fmax+1):\n",
    "    \n",
    "    # reset weights to initial (common initialization for all folds)\n",
    "    model.set_weights(Winit)\n",
    "\n",
    "    tr_paths = []\n",
    "    for sub in folds[fold, 2:4]:\n",
    "        tr_paths = tr_paths + glob.glob(\"./databatches/sub\" + str(sub+1) + \"*.pickle\")\n",
    "    steps_per_ep = len(tr_paths)\n",
    "\n",
    "    val_paths = []\n",
    "    for sub in folds[fold, 1:2]:\n",
    "        val_paths = val_paths + glob.glob(\"./databatches/sub\" + str(sub+1) + \"*.pickle\")\n",
    "    val_steps = len(val_paths)\n",
    "\n",
    "    test_paths = []\n",
    "    for sub in folds[fold, 0:1]:\n",
    "        test_paths = test_paths + glob.glob(\"./databatches/sub\" + str(sub+1) + \"*.pickle\")\n",
    "\n",
    "    # Call testing history callback\n",
    "    test_history = TestOnBest(test_paths)\n",
    "\n",
    "    # Run training\n",
    "    tr_gen = data_gen(tr_paths)\n",
    "    val_gen = data_gen(val_paths)\n",
    "    history = model.fit_generator(generator=tr_gen, steps_per_epoch=steps_per_ep, \n",
    "                                  epochs=n_epochs, verbose=2, callbacks=[test_history], \n",
    "                                  validation_data=val_gen, validation_steps=val_steps)\n",
    "\n",
    "    # Retreive test set statistics and merge to training statistics log\n",
    "    history.history.update(test_history.history)\n",
    "\n",
    "    # Save training history\n",
    "    fn = '../outputs/'+mdname+'train_hist_fold'+str(fold)+'.pickle'\n",
    "    pickle_out = open(fn,'wb')\n",
    "    pickle.dump(history.history, pickle_out)\n",
    "    pickle_out.close()\n",
    "    fn_hist = fn\n",
    "\n",
    "    # Save confusion matrix\n",
    "    fn = '../outputs/'+mdname+'confusion_matrix_fold'+str(fold)+'.pickle'\n",
    "    pickle_out = open(fn, 'wb')\n",
    "    pickle.dump(test_history.confusion_matrix, pickle_out)\n",
    "    pickle_out.close()\n",
    "    fn_mat = fn\n",
    "\n",
    "    # Save weights after training is finished\n",
    "    model.set_weights(test_history.best_weights)\n",
    "    fn = '../outputs/'+mdname+'weights_fold'+str(fold)+'.hdf5'\n",
    "    model.save_weights(fn)\n",
    "\n",
    "    # Plot loss and accuracy by epoch\n",
    "    plot_training_history(history, fold, plotpath)\n",
    "\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:21:10.431893Z",
     "start_time": "2017-12-21T14:20:33.258427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:\t [4 4 0 4 4 4 4 4 4 4 2 4 1 2 4 1]\n",
      "True class:\t\t [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "with open('./databatches/test_batch.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)\n",
    "preds = model.predict(X)\n",
    "\n",
    "print(\"Predicted class:\\t\", np.argmax(preds, axis=1))\n",
    "print(\"True class:\\t\\t\", np.argmax(y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of predictions using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:21:39.076078Z",
     "start_time": "2017-12-21T14:21:30.108010Z"
    }
   },
   "outputs": [],
   "source": [
    "# create new model with pretrained weights\n",
    "model = build_model()\n",
    "model.load_weights(\"./weights/RNN_5steps_weights_fold4.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:21:39.107459Z",
     "start_time": "2017-12-21T14:21:39.078116Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test data batch from example fold\n",
    "with open('./testbatch/sub11_batch42.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:22:17.016723Z",
     "start_time": "2017-12-21T14:21:39.109260Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate predictions on sample test data\n",
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-21T14:22:17.030593Z",
     "start_time": "2017-12-21T14:22:17.018732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:\t [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "True class:\t\t [2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class:\\t\", np.argmax(preds, axis=1))\n",
    "print(\"True class:\\t\\t\", np.argmax(y, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
